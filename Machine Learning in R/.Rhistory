a <- 5
a = 5
print(b)
a <- b <- (1:10)*2
c <- c(1:10)
d <- c(1:10)
ls()
rm(c)
# To browse, type browseURL(...)
browseURL("http://cran.r-projects.org/web/views/")
# To browse, type browseURL(...)
browseURL("http://cran.r-project.org/web/views/")
# To show all libraries: library()
# To show current loaded libraries: search()
# To install new R Library: install.packages("...")
install.packages("LiblineaR")
# To show all libraries: library()
# To show current loaded libraries: search()
search()
# To import library:
# 1. see tab PACKAGES on right below nav and checklist the checkbox
# 2. execute this code, require("...")
require("LiblineaR")
# To remove library:
# 1. see tab PACKAGES on right below nav and unchecklist the checkbox
# 2. excute this code, detach("...", unload=TRUE)
detach("LiblineaR", unload=TRUE)
# To remove library:
# 1. see tab PACKAGES on right below nav and unchecklist the checkbox
# 2. excute this code, detach("package:...", unload=TRUE)
detach("package:LiblineaR", unload=TRUE)
# To see R library's documentation or help: ? ...
? cluster
# To see R library's documentation or help: ? ...
? stats
data()
str(iris)
iris
# But to load it into Global Environment (for use), type data(...)
data(iris)
e <- sequence(10, 100, by=25)
e <- sequence(5, 100, by=25)
f <- seq(5, 100, by=20)
library(keras)
install_keras()
install_tensorflow()
library(keras)
install_keras()
library(keras)
library(reticulate)
use_python("/usr/local/bin/python")
install_keras()
use_python("/usr/local/bin/python")
install_keras(conda = "C:/TEMP/anaconda/envs/test/Scripts/conda.exe")
install_keras(conda = "C:/TEMP/anaconda/envs/test/Scripts/conda.exe")
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/keras")
tensorflow::install_tensorflow()
tensorflow::tf_config()
keras::install_keras()
keras::dataset_fashion_mnist()
library(keras)
dataset_mnist()
dataset_fashion_mnist()
library(keras)
dataset_fashion_mnist()
fashion_mnist = dataset_fashion_mnist()
View(fashion_mnist)
View(fashion_mnist)
c(x_train, y_train) = fashion_mnist$train
c(x_train, y_train) %<-% fashion_mnist$train
fashion_mnist = dataset_fashion_mnist()
x_train = fashion_mnist$train$x
y_train = fashion_mnist$train$y
x_test = fashion_mnist$test$x
y_test = fashion_mnist$test$y
# See dimension shape
dim(x_train)
# See dimension shape
dim(x_train), dim(y_train)
dim(y_train)
str(x_train)
# Create class_names
class_names = c("T-shirt/top, Trouser/pants, Pullover shirt, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot")
# Create class_names
class_names = c("T-shirt/top", "Trouser/pants", "Pullover shirt", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot")
plot(as.raster(fobject, max=255))
# Take a simple image
fobject = x_train[5, ,] # fifth image with all its pixels
plot(as.raster(fobject, max=255))
# Normalizing
x_train = x_train / 255
x_test = x_test / 255
x_train[5]
x_train[5,,]
x_train[1:5,,]
# See dimension shape
dim(x_train)
# Normalization
x_train = x_train / 255
x_test = x_test / 255
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
# Take a simple image
fobject = x_train[5,,] # fifth image with all its pixels
plot(as.raster(fobject, max=255)) # as raster image
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
# Import KERAS Library
library(keras)
# Step 1: TRAIN THE DATA
model = keras_model_sequential()
help(fit)
help(keras.fit)
model = keras_model_sequential()
model %>%
layer_flatten(input_shape=c(28, 28)) %>%
layer_dense(units=128, activation='relu') %>%
layer_dense(units=10, activation='softmax')
# Step 2: FIT AND COMPILE
model %>% fit(
x_train, y_train,
epochs=30, batch_size=1,
validation_data=list(x_val, y_val)
)
model %>% compile(
optimizer='sgd',
loss='sparse_categorical_crossentropy',
metrics=c('accuracy')
)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
fashion_mnist = dataset_fashion_mnist()
# Another way to do test_train_split
# c(x_train, y_train) %<-% fashion_mnist$train
# c(x_test, y_test) %<-% fashion_mnist$test
x_train = fashion_mnist$train$x
y_train = fashion_mnist$train$y
x_test = fashion_mnist$test$x
y_test = fashion_mnist$test$y
# Split train dataset into train and validation data
idx = 1:5000
x_train = x_train[-idx,,]
x_val = x_train[idx,,]
y_train = y_train[-idx]
y_val = y_train[idx]
# See dimension shape
dim(x_train)
dim(y_train)
dim(x_val)
dim(y_val)
dim(x_test)
dim(y_test)
# Create class_names
class_names = c("T-shirt/top", "Trouser/pants", "Pullover shirt", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot")
# Plot a sample image
fobject = x_train[5,,] # fifth image with all its pixels
plot(as.raster(fobject, max=255)) # as raster image
# Normalization
x_train = x_train / 255
x_test = x_test / 255
# Import KERAS Library
library(keras)
# Step 1: DEFINE THE MODEL (Keras Sequential)
model = keras_model_sequential()
model %>%
layer_flatten(input_shape=c(28, 28)) %>%
layer_dense(units=128, activation='relu') %>%
layer_dense(units=10, activation='softmax')
# Step 2: COMPILE AND FIT
model %>% compile(
optimizer='sgd',
loss='sparse_categorical_crossentropy',
metrics=c('accuracy')
)
model %>% fit(
x_train, y_train,
epochs=30, batch_size=100,
validation_data=list(x_val, y_val)
)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
model
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
source('E:/DATA SCIENCE/MACHINE LEARNING/R - Regression/MachineLearning.R', echo=TRUE)
# Step 4: EVALUATE
score = model %>% evaluate(x_test, y_test)
score$loss
score
score.loss
# Step 4: EVALUATE
score = model %>% evaluate(x_test, y_test)
# Step 4: EVALUATE
score <- model %>% evaluate(x_test, y_test)
# Step 4: EVALUATE
score = model %>% evaluate(x_test, y_test)
score[0]
score[,]
score(0)
score
print(score$loss)
print(score.loss)
score(x)$loss
score()$loss
score["loss"]
print("Validation Loss: ", score["val_loss"])
print("Loss: ", score["loss"])
cat(Loss: ", score["loss"])
cat("Loss: ", score["loss"])
cat("Validation Loss: ", score["val_loss"])
score
cat("Accuracy: ", score["accuracy"]*100)
sprintf("Loss: %f", score["loss"]*100)
sprintf("Loss: %.3f %", score["loss"]*100)
sprintf("Loss: %.3f", score["loss"]*100)
sprintf("Loss: %.3f", score["loss"]*100, "%")
sprintf("Loss: %.3f n", score["loss"]*100)
sprintf("Loss: %.3f %", score["loss"]*100)
sprintf("Loss: %.3f \%", score["loss"]*100)
sprintf("Loss: %.3f \\%", score["loss"]*100)
sprintf("Loss: %.3f /%", score["loss"]*100)
sprintf("Loss: %.3f percent", score["loss"]*100)
sprintf("Loss: %.3f percent '%'", score["loss"]*100)
paste0(sprintf("Loss: %.3f", score["loss"]*100), '%')
paste0(sprintf("Accuracy: %.3f", score["accuracy"]*100), '%')
paste0(sprintf("Loss: %.3f", score["loss"]*100), '%')
paste0(sprintf("Accuracy: %.3f", score["accuracy"]*100), '%')
# Step 5: PREDICTION
predict = model %>% predict(x_test)
predict
class_names[which.max(predict[1,])]
class_names[which.max(predict[2,])]
class_names[which.max(predict[1,])]
predict
predict[1, ]
predict
dim(predict)
predict[0, ] # probability of
predict[1, ] # probability of
plot(as.raster(x_test[1,,], max=255))
plot(as.raster(x_test[1,,], max=1))
model %>% predict_classes(x_test)
class_predict = model %>% predict_classes(x_test)
dim(class_predict)
class_predict[1]
predict[1] # show 10 prediction probability of each class on the first image
predict[1, ] # show 10 prediction probability of each class on the first image
class_predict[3] # show that the third image is classified as class =
class_names[class_predict[1]]
which.max(predict[1,]) # find the maximum prediction probability
class_names[which.max(predict[1,])] # get class name in string
class_names[class_predict[3]+1]
library(keras)
house = dataset_boston_housing()
View(house)
View(house)
x_train = house$train$x
y_train = house$train$y
x_test = house$test$x
y_test = house$test$y
# Normalize x_train
x_train = scale(x_train)
# Normalize x_test using mean & stdev from x_train
x_test = scale(x_test, center=attr(x_train, "scaled:center"), scale=attr(x_train, "scaled:scale"))
dim(x_train)
dim(x_train)[2]
View(x_train)
View(x_train)
# REGRESSION USING KERAS FUNCTIONAL API
# Step 1: DEFINE THE MODEL (Keras Functional consists of input+output)
inputs = layer_input(shape=dim(x_train)[2]) # take the second dimension since it represents total of variables == 13
target_output = inputs %>%
layer_dense(64, activation='relu') %>%
layer_dense(64, activation='relu') %>%
layer_dense(1)
# Step 2: COMPILE THE MODEL
reg_model = keras_model(inputs=inputs, outputs=target_output)
reg_model %>% compile(
optimizer='rmsprop',
loss='mse',
metrics=list('mean_absolute_error')
)
# Step 3: FIT THE MODEL
reg_model %>% fit(x_train, y_train, epochs=30, batch_size=100)
# Step 4: EVALUATE WITH TEST DATA
score = reg_model %>% evaluate(x_test, y_test)
paste0(sprintf("Loss: %.3f", score["loss"]*100), '%')
paste0(sprintf("Accuracy: %.3f", score["accuracy"]*100), '%')
score
paste0(sprintf("Mean Abs Error: %.3f", score["mean_absolute_error"]*100), '%')
paste0(sprintf("Loss: %.3f", score["loss"]*100), '%')
score
sprintf("Mean Abs Error: %.3f", score["mean_absolute_error"])
sprintf("Loss: %.3f", score["loss"])
summary(model) # see summary of the model, or summary(model)
summary(reg_model)
reg_model
source('E:/DATA SCIENCE/MACHINE LEARNING/Machine Learning in R/Regression.R', echo=TRUE)
# SAVE THE MODEL
reg_model %>% save_model_hdf5("model/regression-model-in-R.h5")
# SAVE THE MODEL
reg_model %>% save_model_hdf5("model/regression-model-in-R.h5")
# SAVE THE MODEL
reg_model %>% save_model_hdf5("model/regression-model-in-R.h5")
setwd("E:/DATA SCIENCE/MACHINE LEARNING/Machine Learning in R")
# SAVE THE MODEL
reg_model %>% save_model_hdf5("model/regression-model-in-R.h5")
# RELOAD THE MODEL
reg_model_reload = load_model_hdf5("model/regression-model-in-R.h5")
rm(reg_model_reload())
rm(list=reg_model_reload())
# SAVE THE MODEL
reg_model %>% save_model_hdf5(filepath="model", object="regression-model-in-R.h5")
# SAVE THE MODEL
reg_model %>% save_model_hdf5(filepath="model", object="regression-model-in-R.h5")
# SAVE THE MODEL
reg_model %>% save_model_hdf5("model/regression-model-in-R.h5")
# SAVE THE MODEL
reg_model %>% save_model_hdf5("model/regression-model-in-R.h5")
df = read.csv("data/House_Price.csv", header=TRUE)
df = read.csv("data/House_Price.csv", header=TRUE)
summary(df)
str(df)
hist(df$airport)
hist(df$crime_rate)
barplot(table(df$airport))
which(is.na(df))
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] = mean(df$n_hos_beds, na.rm=TRUE)
which(is.na(df$n_hos_beds))
install.packages("dummies")
library(dummies)
df = dummy.data.frame(df)
View(df)
View(df)
